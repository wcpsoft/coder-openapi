server:
  host: 0.0.0.0
  port: 8080
  workers: 10
  shutdown_timeout: 30

models_cache_dir: "models_cache"

chat:
  defaults:
    temperature: 0.7
    top_p: 0.9
    n: 1
    max_tokens: 2048
    stream: false

locales:
  path: "locales"
  default: "en"

models:
  yi-coder:
    hf_hub_id: "01-ai/Yi-Coder-1.5B-Chat"
    model_files:
      weights:
        - "model.safetensors"
      config: "config.json"
      tokenizer: "tokenizer.model"
      tokenizer_config: "tokenizer.json"
      generation_config: "generation_config.json"

  deepseek-coder:
    hf_hub_id: "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
    model_files:
      weights:
        - "model-00001-of-000004.safetensors"
        - "model-00002-of-000004.safetensors"
        - "model-00003-of-000004.safetensors"
        - "model-00004-of-000004.safetensors"
      config: "config.json"
      tokenizer: "tokenizer.json"
      tokenizer_config: "tokenizer_config.json"
      generation_config: "generation_config.json"
